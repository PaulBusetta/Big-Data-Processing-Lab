{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WPylkxI7S73"
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAnIgoXgv3FI"
   },
   "source": [
    "### *get data for labs:*\n",
    "!curl -L -o around_the_world_in_80_days.txt https://www.gutenberg.org/ebooks/103.txt.utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "amepofAo0Z88"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "# set env vars for java and spark\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\maeld\\.jdks\\openjdk-21.0.2\" # replace with appropriate path\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\Spark\\spark-4.0.1-bin-hadoop3\" # replace with appropriate path\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\" # replace with appropriate path\n",
    "\n",
    "# start findspark so notebook can interact with spark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zmv2ros75Gnu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mType:\u001b[39m        module\n",
      "\u001b[31mString form:\u001b[39m <module 'findspark' from 'c:\\\\Users\\\\maeld\\\\.vscode\\\\Inge5\\\\Big_Data\\\\Lab1v2\\\\Big-Data-Processing-Lab\\\\.venv\\\\Lib\\\\site-packages\\\\findspark.py'>\n",
      "\u001b[31mFile:\u001b[39m        c:\\users\\maeld\\.vscode\\inge5\\big_data\\lab1v2\\big-data-processing-lab\\.venv\\lib\\site-packages\\findspark.py\n",
      "\u001b[31mSource:\u001b[39m     \n",
      "\u001b[33m\"\"\"Find spark home, and initialize by adding pyspark to sys.path.\u001b[39m\n",
      "\n",
      "\u001b[33mIf SPARK_HOME is defined, it will be used to put pyspark on sys.path.\u001b[39m\n",
      "\u001b[33mOtherwise, common locations for spark will be searched.\u001b[39m\n",
      "\u001b[33m\"\"\"\u001b[39m\n",
      "\n",
      "\u001b[38;5;28;01mfrom\u001b[39;00m glob \u001b[38;5;28;01mimport\u001b[39;00m glob\n",
      "\u001b[38;5;28;01mimport\u001b[39;00m os\n",
      "\u001b[38;5;28;01mimport\u001b[39;00m sys\n",
      "\n",
      "__version__ = \u001b[33m\"2.0.1\"\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m find():\n",
      "    \u001b[33m\"\"\"Find a local spark installation.\u001b[39m\n",
      "\n",
      "\u001b[33m    Will first check the SPARK_HOME env variable, and otherwise\u001b[39m\n",
      "\u001b[33m    search common installation locations, e.g. from homebrew\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "    spark_home = os.environ.get(\u001b[33m\"SPARK_HOME\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m spark_home:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"pyspark\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m sys.modules:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m os.path.dirname(sys.modules[\u001b[33m\"pyspark\"\u001b[39m].__file__)\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;28;01min\u001b[39;00m [\n",
      "            \u001b[33m\"/usr/local/opt/apache-spark/libexec\"\u001b[39m,  \u001b[38;5;66;03m# macOS Homebrew\u001b[39;00m\n",
      "            \u001b[33m\"/usr/lib/spark/\"\u001b[39m,  \u001b[38;5;66;03m# AWS Amazon EMR\u001b[39;00m\n",
      "            \u001b[33m\"/usr/local/spark/\"\u001b[39m,  \u001b[38;5;66;03m# common linux path for spark\u001b[39;00m\n",
      "            \u001b[33m\"/opt/spark/\"\u001b[39m,  \u001b[38;5;66;03m# other common linux path for spark\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# Any other common places to look?\u001b[39;00m\n",
      "        ]:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(path):\n",
      "                spark_home = path\n",
      "                \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m spark_home:\n",
      "        \u001b[38;5;66;03m# last resort: try importing pyspark (pip-installed, already on sys.path)\u001b[39;00m\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mimport\u001b[39;00m pyspark\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m ImportError:\n",
      "            \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            spark_home = os.path.dirname(pyspark.__file__)\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m spark_home:\n",
      "        \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\n",
      "            \u001b[33m\"Couldn't find Spark, make sure SPARK_HOME env is set\"\u001b[39m\n",
      "            \u001b[33m\" or Spark is in an expected location (e.g. from homebrew installation).\"\u001b[39m\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m spark_home\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m _edit_rc(spark_home, sys_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "    \u001b[33m\"\"\"Persists changes to environment by changing shell config.\u001b[39m\n",
      "\n",
      "\u001b[33m    Adds lines to .bashrc to set environment variables\u001b[39m\n",
      "\u001b[33m    including the adding of dependencies to the system path. Will only\u001b[39m\n",
      "\u001b[33m    edit this file if they already exist. Currently only works for bash.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    spark_home : str\u001b[39m\n",
      "\u001b[33m        Path to Spark installation.\u001b[39m\n",
      "\u001b[33m    sys_path: list(str)\u001b[39m\n",
      "\u001b[33m        Paths (if any) to be added to $PYTHONPATH.\u001b[39m\n",
      "\u001b[33m        Should include python subdirectory of Spark installation, py4j\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    bashrc_location = os.path.expanduser(\u001b[33m\"~/.bashrc\"\u001b[39m)\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(bashrc_location):\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m open(bashrc_location, \u001b[33m\"a\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m bashrc:\n",
      "            bashrc.write(\u001b[33m\"\\n# Added by findspark\\n\"\u001b[39m)\n",
      "            bashrc.write(\u001b[33m\"export SPARK_HOME={}\\n\"\u001b[39m.format(spark_home))\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m sys_path:\n",
      "                bashrc.write(\n",
      "                    \u001b[33m\"export PYTHONPATH={}\\n\"\u001b[39m.format(\n",
      "                        os.pathsep.join(sys_path + [\u001b[33m\"$PYTHONPATH\"\u001b[39m])\n",
      "                    )\n",
      "                )\n",
      "            bashrc.write(\u001b[33m\"\\n\"\u001b[39m)\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m _edit_ipython_profile(spark_home, sys_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "    \u001b[33m\"\"\"Adds a startup file to the current IPython profile to import pyspark.\u001b[39m\n",
      "\n",
      "\u001b[33m    The startup file sets the required environment variables and imports pyspark.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    spark_home : str\u001b[39m\n",
      "\u001b[33m        Path to Spark installation.\u001b[39m\n",
      "\u001b[33m    sys_path : list(str)\u001b[39m\n",
      "\u001b[33m        Paths to be added to sys.path.\u001b[39m\n",
      "\u001b[33m        Should include python subdirectory of Spark installation, py4j\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "    \u001b[38;5;28;01mfrom\u001b[39;00m IPython \u001b[38;5;28;01mimport\u001b[39;00m get_ipython\n",
      "\n",
      "    ip = get_ipython()\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m ip:\n",
      "        profile_dir = ip.profile_dir.location\n",
      "    \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mfrom\u001b[39;00m IPython.utils.path \u001b[38;5;28;01mimport\u001b[39;00m locate_profile\n",
      "\n",
      "        profile_dir = locate_profile()\n",
      "\n",
      "    startup_file_loc = os.path.join(profile_dir, \u001b[33m\"startup\"\u001b[39m, \u001b[33m\"findspark.py\"\u001b[39m)\n",
      "\n",
      "    \u001b[38;5;28;01mwith\u001b[39;00m open(startup_file_loc, \u001b[33m\"w\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m startup_file:\n",
      "        \u001b[38;5;66;03m# Lines of code to be run when IPython starts\u001b[39;00m\n",
      "        startup_file.write(\u001b[33m\"import sys, os\\n\"\u001b[39m)\n",
      "        startup_file.write(\u001b[33m\"os.environ['SPARK_HOME'] = {}\\n\"\u001b[39m.format(repr(spark_home)))\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m sys_path:\n",
      "            startup_file.write(\u001b[33m\"sys.path[:0] = {}\\n\"\u001b[39m.format(repr(sys_path)))\n",
      "        startup_file.write(\u001b[33m\"import pyspark\\n\"\u001b[39m)\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m init(spark_home=\u001b[38;5;28;01mNone\u001b[39;00m, python_path=\u001b[38;5;28;01mNone\u001b[39;00m, edit_rc=\u001b[38;5;28;01mFalse\u001b[39;00m, edit_profile=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "    \u001b[33m\"\"\"Make pyspark importable.\u001b[39m\n",
      "\n",
      "\u001b[33m    Sets environment variables and adds dependencies to sys.path.\u001b[39m\n",
      "\u001b[33m    If no Spark location is provided, will try to find an installation.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    spark_home : str, optional, default = None\u001b[39m\n",
      "\u001b[33m        Path to Spark installation, will try to find automatically\u001b[39m\n",
      "\u001b[33m        if not provided.\u001b[39m\n",
      "\u001b[33m    python_path : str, optional, default = None\u001b[39m\n",
      "\u001b[33m        Path to Python for Spark workers (PYSPARK_PYTHON),\u001b[39m\n",
      "\u001b[33m        will use the currently running Python if not provided.\u001b[39m\n",
      "\u001b[33m    edit_rc : bool, optional, default = False\u001b[39m\n",
      "\u001b[33m        Whether to attempt to persist changes by appending to shell\u001b[39m\n",
      "\u001b[33m        config.\u001b[39m\n",
      "\u001b[33m    edit_profile : bool, optional, default = False\u001b[39m\n",
      "\u001b[33m        Whether to create an IPython startup file to automatically\u001b[39m\n",
      "\u001b[33m        configure and import pyspark.\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m spark_home:\n",
      "        spark_home = find()\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m python_path:\n",
      "        python_path = os.environ.get(\u001b[33m\"PYSPARK_PYTHON\"\u001b[39m, sys.executable)\n",
      "\n",
      "    \u001b[38;5;66;03m# ensure SPARK_HOME is defined\u001b[39;00m\n",
      "    os.environ[\u001b[33m\"SPARK_HOME\"\u001b[39m] = spark_home\n",
      "\n",
      "    \u001b[38;5;66;03m# ensure PYSPARK_PYTHON is defined\u001b[39;00m\n",
      "    os.environ[\u001b[33m\"PYSPARK_PYTHON\"\u001b[39m] = python_path\n",
      "\n",
      "    \u001b[38;5;66;03m# add pyspark to sys.path\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"pyspark\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m sys.modules:\n",
      "        spark_python = os.path.join(spark_home, \u001b[33m\"python\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            py4j = glob(os.path.join(spark_python, \u001b[33m\"lib\"\u001b[39m, \u001b[33m\"py4j-*.zip\"\u001b[39m))[\u001b[32m0\u001b[39m]\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m IndexError:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m Exception(\n",
      "                \u001b[33m\"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\"\u001b[39m.format(\n",
      "                    spark_python\n",
      "                )\n",
      "            )\n",
      "        sys.path[:\u001b[32m0\u001b[39m] = sys_path = [spark_python, py4j]\n",
      "    \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "        \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "        sys_path = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m edit_rc:\n",
      "        _edit_rc(spark_home, sys_path)\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m edit_profile:\n",
      "        _edit_ipython_profile(spark_home, sys_path)\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m _add_to_submit_args(to_add):\n",
      "    \u001b[33m\"\"\"Add string s to the PYSPARK_SUBMIT_ARGS env var\"\"\"\u001b[39m\n",
      "    existing_args = os.environ.get(\u001b[33m\"PYSPARK_SUBMIT_ARGS\"\u001b[39m, \u001b[33m\"\"\u001b[39m)\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m existing_args:\n",
      "        \u001b[38;5;66;03m# if empty, start with default pyspark-shell\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# ref: pyspark.java_gateway.launch_gateway\u001b[39;00m\n",
      "        existing_args = \u001b[33m\"pyspark-shell\"\u001b[39m\n",
      "    \u001b[38;5;66;03m# add new args to front to avoid insert after executable\u001b[39;00m\n",
      "    submit_args = \u001b[33m\"{} {}\"\u001b[39m.format(to_add, existing_args)\n",
      "    os.environ[\u001b[33m\"PYSPARK_SUBMIT_ARGS\"\u001b[39m] = submit_args\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m submit_args\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m add_packages(packages):\n",
      "    \u001b[33m\"\"\"Add external packages to the pyspark interpreter.\u001b[39m\n",
      "\n",
      "\u001b[33m    Set the PYSPARK_SUBMIT_ARGS properly.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    packages: list of package names in string format\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    \u001b[38;5;66;03m# if the parameter is a string, convert to a single element list\u001b[39;00m\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m isinstance(packages, str):\n",
      "        packages = [packages]\n",
      "\n",
      "    _add_to_submit_args(\u001b[33m\"--packages \"\u001b[39m + \u001b[33m\",\"\u001b[39m.join(packages))\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m add_jars(jars):\n",
      "    \u001b[33m\"\"\"Add external jars to the pyspark interpreter.\u001b[39m\n",
      "\n",
      "\u001b[33m    Set the PYSPARK_SUBMIT_ARGS properly.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    jars: list of path to jars in string format\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    \u001b[38;5;66;03m# if the parameter is a string, convert to a single element list\u001b[39;00m\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m isinstance(jars, str):\n",
      "        jars = [jars]\n",
      "\n",
      "    _add_to_submit_args(\u001b[33m\"--jars \"\u001b[39m + \u001b[33m\",\"\u001b[39m.join(jars))"
     ]
    }
   ],
   "source": [
    "# what does findspark do? use the ?? magic command to find out\n",
    "# Note 1: in colab, this may open in a side panel\n",
    "# Note 2: this magic command is often helpful when encountering an object in a\n",
    "# notebook that is unfamiliar. More information will be displayed if it exists\n",
    "?? findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyrzgODB7h-I"
   },
   "source": [
    "# 1. Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubaJBAcmHN7w"
   },
   "source": [
    "Instructions:  \n",
    "For each cell marked \"double-click and add explanation here\" please answer the question in your own words.  \n",
    "In the section where you complete the code to perform basic nlp text cleaning and exploration tasks, the goal is to chain all of the transformations together in a single function. For learning and exploration purposes, it is acceptable to have each step seperate, but the last cell in this section should be one function with all transformations chained together.  \n",
    "For steps c and f, it is acceptable to use your favorite chatbot to generate a list of common stop words (c) and punctuation (e) for use in the code. As these are common steps in nlp/text processing tasks, there are pleanty of libraries to help with this such as nltk, but there is no need to import extra dependencies for this lab unless you are already familiar with working with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sT9v8V8z3xRh"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"word_count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WQuzr4jFgwA8"
   },
   "outputs": [],
   "source": [
    "# Defind the rdd\n",
    "rdd = sc.textFile('../data/around_the_world_in_80_days.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eDOA5KkhhEpe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of Around the World in Eighty Days',\n",
       " '    ',\n",
       " 'This ebook is for the use of anyone anywhere in the United States and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever. You may copy it, give it away or re-use it under the terms',\n",
       " 'of the Project Gutenberg License included with this ebook or online',\n",
       " 'at www.gutenberg.org. If you are not located in the United States,',\n",
       " 'you will have to check the laws of the country where you are located',\n",
       " 'before using this eBook.',\n",
       " '',\n",
       " 'Title: Around the World in Eighty Days',\n",
       " '',\n",
       " 'Author: Jules Verne',\n",
       " '',\n",
       " 'Release date: January 1, 1994 [eBook #103]',\n",
       " '                Most recently updated: October 29, 2024',\n",
       " '',\n",
       " 'Language: English',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the first x lines of the rdd\n",
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-Ygmzp9YhIuc"
   },
   "outputs": [],
   "source": [
    "# example lambda function\n",
    "words = rdd.flatMap(lambda lines: lines.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cx0OTFbfhTmg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:56"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note and explain the output of the below command\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ebu8bJo8yTa"
   },
   "source": [
    "double-click and add explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSFM2oMV8a4l"
   },
   "source": [
    "<ADD EXPLANATIONâ€¯HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ0xArEv8u_0"
   },
   "outputs": [],
   "source": [
    "# Note and explain the output of the following command, focusing on the difference with the\n",
    "# above command\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8xJlKEB9aDY"
   },
   "source": [
    "double-click and add explanation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eD96cn2NhUaZ"
   },
   "outputs": [],
   "source": [
    "# nicer print\n",
    "for w in words.collect():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tQ4EUF7hyDx"
   },
   "outputs": [],
   "source": [
    "# Print first x words\n",
    "words.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6M0MIGmEVrq"
   },
   "outputs": [],
   "source": [
    "# Use cell magic command to help understand what the rdd.flatMap function is doing in the next cell.\n",
    "# Insert a text/markdown cell and explain in your own words.\n",
    "\n",
    "?? rdd.flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdkOE8Pzh4sV"
   },
   "outputs": [],
   "source": [
    "# Initialize a word counter by creating a tuple with word and cound of 1\n",
    "words = rdd.flatMap(lambda lines: lines.split(' ')) \\\n",
    "                    .map(lambda word: (word, 1))\n",
    "\n",
    "for w in words.collect():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFu72I93_8CS"
   },
   "outputs": [],
   "source": [
    "# a. count the occurence of each word\n",
    "\n",
    "counts = words.reduceByKey(operator.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYAIt8A8AGGn"
   },
   "outputs": [],
   "source": [
    "# b. a common first step in text analysis, change all capital letters to lower case\n",
    "\n",
    "words = words.map(lambda wc: (wc[0].lower(), wc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNr23N3MAVla"
   },
   "outputs": [],
   "source": [
    "# c. eliminate the stop words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjOm9cn7B5lI"
   },
   "outputs": [],
   "source": [
    "# d. sort in alphabetical order\n",
    "\n",
    "sorted_counts = counts.sortByKey(ascending=True)\n",
    "sorted_counts.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXrqvUwuCD5E"
   },
   "outputs": [],
   "source": [
    "# e. sort descending by word frequency\n",
    "\n",
    "for w, c in counts.sortBy(lambda x: x[1], ascending=False).take(20):\n",
    "    print(w, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJr1n25lCN2B"
   },
   "outputs": [],
   "source": [
    "# f. remove punctuations and blank spaces\n",
    "\n",
    "clean_tokens = (rdd\n",
    "  .map(lambda line: re.sub(r\"[^\\w]+\", \" \", line.lower()))  # replace non-word chars with space\n",
    "  .flatMap(lambda line: line.split())                      # split on whitespace\n",
    "  .filter(lambda w: w)                                     # drop empties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81MYw5zXCv8X"
   },
   "source": [
    "# 2. What does the following cell block do?\n",
    "Comment the code below line by line after the provided hash-tag. You should be able to explain each line while respecting the pep8 style guide of 79 characters or less per line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrrNF7seCaZ2"
   },
   "outputs": [],
   "source": [
    " # Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "(\"TD\", 35), (\"Brooke\", 25)])\n",
    "\n",
    "# Try to undestand what this code does (line by line)\n",
    "agesRDD = (dataRDD\n",
    "  #\n",
    "  .map(lambda x: (x[0], (x[1], 1)))\n",
    "  #\n",
    "  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "  #\n",
    "  .map(lambda x: (x[0], x[1][0]/x[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzas64DSRt_a"
   },
   "source": [
    "# Where to go from here\n",
    "\n",
    "Further exploration for students who complete the lab before the end of the session or want to go further.\n",
    "\n",
    "- perform eda on the original french version of the [book](https://www.gutenberg.org/ebooks/46541.txt.utf-8) and compare the two\n",
    "- recomplete the exercises using a the docker install\n",
    "- install java and spark directly onto host machine and either rexplore this notebook or perform eda on other data sets\n",
    "- write a simple python timer function for seeing how quickly your rdd runs as written. change the order of the steps in order to make the rdd run as optimally as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XH329uSEU0s_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Big-Data-Processing-Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
