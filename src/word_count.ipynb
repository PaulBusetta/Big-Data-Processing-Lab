{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WPylkxI7S73"
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "amepofAo0Z88"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# start findspark so notebook can interact with spark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "Zmv2ros75Gnu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        module\n",
      "\u001b[0;31mString form:\u001b[0m <module 'findspark' from '/opt/anaconda3/lib/python3.12/site-packages/findspark.py'>\n",
      "\u001b[0;31mFile:\u001b[0m        /opt/anaconda3/lib/python3.12/site-packages/findspark.py\n",
      "\u001b[0;31mSource:\u001b[0m     \n",
      "\u001b[0;34m\"\"\"Find spark home, and initialize by adding pyspark to sys.path.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34mIf SPARK_HOME is defined, it will be used to put pyspark on sys.path.\u001b[0m\n",
      "\u001b[0;34mOtherwise, common locations for spark will be searched.\u001b[0m\n",
      "\u001b[0;34m\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.0.1\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Find a local spark installation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Will first check the SPARK_HOME env variable, and otherwise\u001b[0m\n",
      "\u001b[0;34m    search common installation locations, e.g. from homebrew\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mspark_home\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SPARK_HOME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"pyspark\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pyspark\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"/usr/local/opt/apache-spark/libexec\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# macOS Homebrew\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"/usr/lib/spark/\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# AWS Amazon EMR\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"/usr/local/spark/\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# common linux path for spark\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"/opt/spark/\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# other common linux path for spark\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Any other common places to look?\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mspark_home\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# last resort: try importing pyspark (pip-installed, already on sys.path)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mspark_home\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"Couldn't find Spark, make sure SPARK_HOME env is set\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\" or Spark is in an expected location (e.g. from homebrew installation).\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0m_edit_rc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Persists changes to environment by changing shell config.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Adds lines to .bashrc to set environment variables\u001b[0m\n",
      "\u001b[0;34m    including the adding of dependencies to the system path. Will only\u001b[0m\n",
      "\u001b[0;34m    edit this file if they already exist. Currently only works for bash.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    spark_home : str\u001b[0m\n",
      "\u001b[0;34m        Path to Spark installation.\u001b[0m\n",
      "\u001b[0;34m    sys_path: list(str)\u001b[0m\n",
      "\u001b[0;34m        Paths (if any) to be added to $PYTHONPATH.\u001b[0m\n",
      "\u001b[0;34m        Should include python subdirectory of Spark installation, py4j\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbashrc_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/.bashrc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbashrc_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbashrc_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbashrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbashrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n# Added by findspark\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbashrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"export SPARK_HOME={}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0msys_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mbashrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"export PYTHONPATH={}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpathsep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"$PYTHONPATH\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbashrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0m_edit_ipython_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Adds a startup file to the current IPython profile to import pyspark.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The startup file sets the required environment variables and imports pyspark.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    spark_home : str\u001b[0m\n",
      "\u001b[0;34m        Path to Spark installation.\u001b[0m\n",
      "\u001b[0;34m    sys_path : list(str)\u001b[0m\n",
      "\u001b[0;34m        Paths to be added to sys.path.\u001b[0m\n",
      "\u001b[0;34m        Should include python subdirectory of Spark installation, py4j\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprofile_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocate_profile\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprofile_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocate_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstartup_file_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"startup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"findspark.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartup_file_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstartup_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Lines of code to be run when IPython starts\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstartup_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"import sys, os\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstartup_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"os.environ['SPARK_HOME'] = {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msys_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstartup_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sys.path[:0] = {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstartup_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"import pyspark\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medit_rc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medit_profile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Make pyspark importable.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Sets environment variables and adds dependencies to sys.path.\u001b[0m\n",
      "\u001b[0;34m    If no Spark location is provided, will try to find an installation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    spark_home : str, optional, default = None\u001b[0m\n",
      "\u001b[0;34m        Path to Spark installation, will try to find automatically\u001b[0m\n",
      "\u001b[0;34m        if not provided.\u001b[0m\n",
      "\u001b[0;34m    python_path : str, optional, default = None\u001b[0m\n",
      "\u001b[0;34m        Path to Python for Spark workers (PYSPARK_PYTHON),\u001b[0m\n",
      "\u001b[0;34m        will use the currently running Python if not provided.\u001b[0m\n",
      "\u001b[0;34m    edit_rc : bool, optional, default = False\u001b[0m\n",
      "\u001b[0;34m        Whether to attempt to persist changes by appending to shell\u001b[0m\n",
      "\u001b[0;34m        config.\u001b[0m\n",
      "\u001b[0;34m    edit_profile : bool, optional, default = False\u001b[0m\n",
      "\u001b[0;34m        Whether to create an IPython startup file to automatically\u001b[0m\n",
      "\u001b[0;34m        configure and import pyspark.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mspark_home\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpython_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYSPARK_PYTHON\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# ensure SPARK_HOME is defined\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_HOME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# ensure PYSPARK_PYTHON is defined\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PYSPARK_PYTHON\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# add pyspark to sys.path\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;34m\"pyspark\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mspark_python\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# already imported, no need to patch sys.path\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msys_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0medit_rc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0m_edit_rc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0medit_profile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0m_edit_ipython_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0m_add_to_submit_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Add string s to the PYSPARK_SUBMIT_ARGS env var\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mexisting_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYSPARK_SUBMIT_ARGS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexisting_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# if empty, start with default pyspark-shell\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# ref: pyspark.java_gateway.launch_gateway\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mexisting_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pyspark-shell\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# add new args to front to avoid insert after executable\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubmit_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PYSPARK_SUBMIT_ARGS\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmit_args\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msubmit_args\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0madd_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Add external packages to the pyspark interpreter.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Set the PYSPARK_SUBMIT_ARGS properly.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    packages: list of package names in string format\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# if the parameter is a string, convert to a single element list\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_add_to_submit_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--packages \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0madd_jars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Add external jars to the pyspark interpreter.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Set the PYSPARK_SUBMIT_ARGS properly.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    jars: list of path to jars in string format\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# if the parameter is a string, convert to a single element list\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mjars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_add_to_submit_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--jars \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# what does findspark do? use the ?? magic command to find out\n",
    "# Note 1: in colab, this may open in a side panel\n",
    "# Note 2: this magic command is often helpful when encountering an object in a\n",
    "# notebook that is unfamiliar. More information will be displayed if it exists\n",
    "?? findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyrzgODB7h-I"
   },
   "source": [
    "# 1. Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubaJBAcmHN7w"
   },
   "source": [
    "Instructions:  \n",
    "For each cell marked \"double-click and add explanation here\" please answer the question in your own words.  \n",
    "In the section where you complete the code to perform basic nlp text cleaning and exploration tasks, the goal is to chain all of the transformations together in a single function. For learning and exploration purposes, it is acceptable to have each step seperate, but the last cell in this section should be one function with all transformations chained together.  \n",
    "For steps c and f, it is acceptable to use your favorite chatbot to generate a list of common stop words (c) and punctuation (e) for use in the code. As these are common steps in nlp/text processing tasks, there are pleanty of libraries to help with this such as nltk, but there is no need to import extra dependencies for this lab unless you are already familiar with working with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "sT9v8V8z3xRh"
   },
   "outputs": [],
   "source": [
    "# start a spark session and create spark context for making rdd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"word_count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "WQuzr4jFgwA8"
   },
   "outputs": [],
   "source": [
    "# Defind the rdd\n",
    "# Data is read line by line\n",
    "rdd = sc.textFile('../data/data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "eDOA5KkhhEpe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Around the World in Eighty Days',\n",
       " '',\n",
       " 'by Jules Verne',\n",
       " '',\n",
       " '',\n",
       " 'Contents',\n",
       " '',\n",
       " ' CHAPTER I. IN WHICH PHILEAS FOGG AND PASSEPARTOUT ACCEPT EACH OTHER, THE ONE AS MASTER, THE OTHER AS MAN',\n",
       " ' CHAPTER II. IN WHICH PASSEPARTOUT IS CONVINCED THAT HE HAS AT LAST FOUND HIS IDEAL',\n",
       " ' CHAPTER III. IN WHICH A CONVERSATION TAKES PLACE WHICH SEEMS LIKELY TO COST PHILEAS FOGG DEAR',\n",
       " ' CHAPTER IV. IN WHICH PHILEAS FOGG ASTOUNDS PASSEPARTOUT, HIS SERVANT',\n",
       " ' CHAPTER V. IN WHICH A NEW SPECIES OF FUNDS, UNKNOWN TO THE MONEYED MEN, APPEARS ON ’CHANGE',\n",
       " ' CHAPTER VI. IN WHICH FIX, THE DETECTIVE, BETRAYS A VERY NATURAL IMPATIENCE',\n",
       " ' CHAPTER VII. WHICH ONCE MORE DEMONSTRATES THE USELESSNESS OF PASSPORTS AS AIDS TO DETECTIVES',\n",
       " ' CHAPTER VIII. IN WHICH PASSEPARTOUT TALKS RATHER MORE, PERHAPS, THAN IS PRUDENT',\n",
       " ' CHAPTER IX. IN WHICH THE RED SEA AND THE INDIAN OCEAN PROVE PROPITIOUS TO THE DESIGNS OF PHILEAS FOGG',\n",
       " ' CHAPTER X. IN WHICH PASSEPARTOUT IS ONLY TOO GLAD TO GET OFF WITH THE LOSS OF HIS SHOES',\n",
       " ' CHAPTER XI. IN WHICH PHILEAS FOGG SECURES A CURIOUS MEANS OF CONVEYANCE AT A FABULOUS PRICE',\n",
       " ' CHAPTER XII. IN WHICH PHILEAS FOGG AND HIS COMPANIONS VENTURE ACROSS THE INDIAN FORESTS, AND WHAT ENSUED',\n",
       " ' CHAPTER XIII. IN WHICH PASSEPARTOUT RECEIVES A NEW PROOF THAT FORTUNE FAVORS THE BRAVE']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the first 20 lines of the rdd\n",
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "-Ygmzp9YhIuc"
   },
   "outputs": [],
   "source": [
    "# example lambda function\n",
    "words = rdd.flatMap(lambda lines: lines.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "cx0OTFbfhTmg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[288] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note and explain the output of the below command\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ebu8bJo8yTa"
   },
   "source": [
    "We are using flatMap to map the result into multiples outputs instead of a single output as the function map does\n",
    "Each line will be divided into word and the variable \"words\" will contains all of them, it is a new RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSFM2oMV8a4l"
   },
   "source": [
    "<ADD EXPLANATION HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "mZ0xArEv8u_0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Around', 1),\n",
       " ('the', 1),\n",
       " ('World', 1),\n",
       " ('in', 1),\n",
       " ('Eighty', 1),\n",
       " ('Days', 1),\n",
       " ('', 1),\n",
       " ('by', 1),\n",
       " ('Jules', 1),\n",
       " ('Verne', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('Contents', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('CHAPTER', 1),\n",
       " ('I.', 1),\n",
       " ('IN', 1),\n",
       " ('WHICH', 1),\n",
       " ('PHILEAS', 1),\n",
       " ('FOGG', 1),\n",
       " ('AND', 1),\n",
       " ('PASSEPARTOUT', 1),\n",
       " ('ACCEPT', 1),\n",
       " ('EACH', 1),\n",
       " ('OTHER,', 1),\n",
       " ('THE', 1),\n",
       " ('ONE', 1),\n",
       " ('AS', 1),\n",
       " ('MASTER,', 1),\n",
       " ('THE', 1),\n",
       " ('OTHER', 1),\n",
       " ('AS', 1),\n",
       " ('MAN', 1),\n",
       " ('', 1),\n",
       " ('CHAPTER', 1),\n",
       " ('II.', 1),\n",
       " ('IN', 1),\n",
       " ('WHICH', 1),\n",
       " ('PASSEPARTOUT', 1),\n",
       " ('IS', 1),\n",
       " ('CONVINCED', 1),\n",
       " ('THAT', 1),\n",
       " ('HE', 1),\n",
       " ('HAS', 1),\n",
       " ('AT', 1),\n",
       " ('LAST', 1),\n",
       " ('FOUND', 1),\n",
       " ('HIS', 1),\n",
       " ('IDEAL', 1)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note and explain the output of the following command, focusing on the difference with the\n",
    "# above command\n",
    "words.collect()[:50] #50 to limit output in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8xJlKEB9aDY"
   },
   "source": [
    "The function collect() is used to turn a RDD into an array of object Row(). This function is considered as an action, it asks all the executors to gather data and bring them to the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "O6M0MIGmEVrq"
   },
   "outputs": [],
   "source": [
    "# Use cell magic command to help understand what the rdd.flatMap function is doing in the next cell.\n",
    "# Insert a text/markdown cell and explain in your own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "BdkOE8Pzh4sV"
   },
   "outputs": [],
   "source": [
    "# Initialize a word counter by creating a tuple with word and cound of 1\n",
    "words = rdd.flatMap(lambda lines: lines.split(' ')) \\\n",
    "                    .map(lambda word: (word, 1))\n",
    "\n",
    "#remove the print statement to avoid flooding the output\n",
    "#for w in words.collect(): --- IGNORE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "hFu72I93_8CS"
   },
   "outputs": [],
   "source": [
    "# a. count the occurence of each word\n",
    "# We start by mapping the line to get a tuple with (word, 1) to easily count the occurence by making a sum in the following step\n",
    "# We used reduceByKey to aggregate our data, it take in parameter a lambda function with key, value as inputs\n",
    "\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 1), 4306),\n",
       " (('World', 1), 1),\n",
       " (('in', 1), 987),\n",
       " (('Eighty', 1), 1),\n",
       " (('Days', 1), 1),\n",
       " (('Jules', 1), 1),\n",
       " (('Verne', 1), 1),\n",
       " (('Contents', 1), 1),\n",
       " (('CHAPTER', 1), 74),\n",
       " (('WHICH', 1), 76),\n",
       " (('PHILEAS', 1), 36),\n",
       " (('AND', 1), 17),\n",
       " (('PASSEPARTOUT', 1), 20),\n",
       " (('ACCEPT', 1), 2),\n",
       " (('OTHER,', 1), 2),\n",
       " (('THE', 1), 62),\n",
       " (('ONE', 1), 2),\n",
       " (('AS', 1), 6),\n",
       " (('IS', 1), 16),\n",
       " (('CONVINCED', 1), 2)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "dYAIt8A8AGGn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['around', 'the', 'world', 'in', 'eighty', 'days', '', 'by', 'jules', 'verne']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. a common first step in text analysis, change all capital letters to lower case\n",
    "# In the same way as before, we use a lambda function to handle each word and apply the lower casing operation\n",
    "\n",
    "words_lower = rdd.flatMap(lambda lines: lines.split(' ')).map(lambda w : w.lower())\n",
    "words_lower.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "CNr23N3MAVla"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['around', 'world', 'eighty', 'days', '', 'jules', 'verne', '', '', 'contents']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c. eliminate the stop words.\n",
    "# For this task, we use the nltk library to get a list of stop words and then filter them out of the previous rdd\n",
    "# We can see 'the' disappeared from the output\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words_no_stopw = words_lower.filter(lambda w: w not in stop_words)\n",
    "words_no_stopw.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "HjOm9cn7B5lI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$5,000)',\n",
       " '&c.,',\n",
       " '($1',\n",
       " '(801)',\n",
       " '(a)',\n",
       " '(and',\n",
       " '(any',\n",
       " '(b)',\n",
       " '(c)',\n",
       " '(does']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d. sort in alphabetical order\n",
    "# Same here, we use the sortBy function and apply the lambda function to each word\n",
    "# We add a filter to remove empty words which could appear in first position\n",
    "\n",
    "words_ordered = words_no_stopw.sortBy(lambda w: w).filter(lambda w: w != '')\n",
    "words_ordered.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "WXrqvUwuCD5E"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 1), 4306),\n",
       " (('', 1), 2149),\n",
       " (('of', 1), 1870),\n",
       " (('and', 1), 1790),\n",
       " (('to', 1), 1689),\n",
       " (('a', 1), 1261),\n",
       " (('in', 1), 987),\n",
       " (('was', 1), 974),\n",
       " (('his', 1), 804),\n",
       " (('he', 1), 720)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e. sort descending by word frequency\n",
    "\n",
    "words_desc = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
    "words_desc.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "AJr1n25lCN2B"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['around', 'the', 'world', 'in', 'eighty', 'days', '', 'by', 'jules', 'verne']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(w) :\n",
    "    return re.sub(r'[^\\w\\s]', '', w)\n",
    "\n",
    "words_no_punct = words_lower.map(remove_punctuation)\n",
    "words_no_punct.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81MYw5zXCv8X"
   },
   "source": [
    "# 2. What does the following cell block do?\n",
    "Comment the code below line by line after the provided hash-tag. You should be able to explain each line while respecting the pep8 style guide of 79 characters or less per line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "FrrNF7seCaZ2"
   },
   "outputs": [],
   "source": [
    " # Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "(\"TD\", 35), (\"Brooke\", 25)]) # converting a list into a RDD\n",
    "\n",
    "# Try to undestand what this code does (line by line)\n",
    "agesRDD = (dataRDD\n",
    "  .map(lambda x: (x[0], (x[1], 1))) # creat a new tuple based on the previous one with this schema : (name, (age, 1))\n",
    "  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) # Aggregate by taking the name as the key and the value as the tuple (age, 1)\n",
    "  # for each, sum of ages and counter\n",
    "  .map(lambda x: (x[0], x[1][0]/x[1][1]))) # mapping again by creating tuple (name, age/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brooke', 22.5), ('Denny', 31.0), ('TD', 35.0), ('Jules', 30.0)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agesRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzas64DSRt_a"
   },
   "source": [
    "# Where to go from here\n",
    "\n",
    "Further exploration for students who complete the lab before the end of the session or want to go further.\n",
    "\n",
    "- perform eda on the original french version of the [book](https://www.gutenberg.org/ebooks/46541.txt.utf-8) and compare the two\n",
    "- recomplete the exercises using a the docker install\n",
    "- install java and spark directly onto host machine and either rexplore this notebook or perform eda on other data sets\n",
    "- write a simple python timer function for seeing how quickly your rdd runs as written. change the order of the steps in order to make the rdd run as optimally as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "XH329uSEU0s_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-18 15:21:23--  https://www.gutenberg.org/cache/epub/46541/pg46541.txt\n",
      "Résolution de www.gutenberg.org (www.gutenberg.org)… 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47\n",
      "Connexion à www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 472772 (462K) [text/plain]\n",
      "Sauvegarde en : « le_tour_du_monde_en_quatre_vingts_jours.txt »\n",
      "\n",
      "le_tour_du_monde_en 100%[===================>] 461,69K  1,25MB/s    ds 0,4s    \n",
      "\n",
      "2025-09-18 15:21:23 (1,25 MB/s) — « le_tour_du_monde_en_quatre_vingts_jours.txt » sauvegardé [472772/472772]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get French data for labs - corrected URL and path\n",
    "!wget -nc -O le_tour_du_monde_en_quatre_vingts_jours.txt https://www.gutenberg.org/cache/epub/46541/pg46541.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. French Version Analysis - Le Tour du Monde en Quatre-Vingts Jours\n",
    "\n",
    "Following the same methodology as the English version, we will now analyze the original French text of \"Around the World in Eighty Days\" by Jules Verne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of Le Tour du monde en quatre-vingts jours',\n",
       " '    ',\n",
       " 'This ebook is for the use of anyone anywhere in the United States and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever. You may copy it, give it away or re-use it under the terms',\n",
       " 'of the Project Gutenberg License included with this ebook or online',\n",
       " 'at www.gutenberg.org. If you are not located in the United States,',\n",
       " 'you will have to check the laws of the country where you are located',\n",
       " 'before using this eBook.',\n",
       " '',\n",
       " 'Title: Le Tour du monde en quatre-vingts jours',\n",
       " '',\n",
       " 'Author: Jules Verne',\n",
       " '',\n",
       " 'Illustrator: Léon Benett',\n",
       " '        Alphonse Marie de Neuville',\n",
       " '',\n",
       " 'Release date: August 9, 2014 [eBook #46541]',\n",
       " '                Most recently updated: October 24, 2024',\n",
       " '']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first lines of the French RDD\n",
    "rdd_french = sc.textFile('le_tour_du_monde_en_quatre_vingts_jours.txt')\n",
    "rdd_french.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg', 1)\n",
      "('eBook', 1)\n",
      "('of', 1)\n",
      "('Le', 1)\n",
      "('Tour', 1)\n",
      "('du', 1)\n",
      "('monde', 1)\n",
      "('en', 1)\n",
      "('quatre-vingts', 1)\n",
      "('jours', 1)\n",
      "('', 1)\n",
      "('', 1)\n",
      "('', 1)\n",
      "('', 1)\n",
      "('', 1)\n",
      "('This', 1)\n",
      "('ebook', 1)\n",
      "('is', 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize French word counter by creating tuples with word and count of 1\n",
    "words_french = rdd_french.flatMap(lambda lines: lines.split(' ')) \\\n",
    "                         .map(lambda word: (word, 1))\n",
    "\n",
    "# Print first 20 French words with their counts\n",
    "for w in words_french.take(20):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gutenberg', 22),\n",
       " ('eBook', 4),\n",
       " ('of', 112),\n",
       " ('Tour', 3),\n",
       " ('du', 763),\n",
       " ('quatre-vingts', 23),\n",
       " ('', 5389),\n",
       " ('for', 22),\n",
       " ('use', 9),\n",
       " ('anyone', 4)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. Count French words with reduceByKey\n",
    "word_counts_french = words_french.reduceByKey(lambda a, b: a + b)\n",
    "word_counts_french.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'le',\n",
       " 'tour',\n",
       " 'du',\n",
       " 'monde',\n",
       " 'en']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. Convert French words to lowercase\n",
    "words_french_lower = rdd_french.flatMap(lambda lines: lines.split(' ')).map(lambda w: w.lower())\n",
    "words_french_lower.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 5389),\n",
       " ('de', 2851),\n",
       " ('le', 1609),\n",
       " ('à', 1597),\n",
       " ('la', 1501),\n",
       " ('et', 1371),\n",
       " ('les', 958),\n",
       " ('du', 763),\n",
       " ('en', 707),\n",
       " ('il', 684)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort French word counts by frequency descending\n",
    "word_counts_french_sorted = word_counts_french.sortBy(lambda x: x[1], ascending=False)\n",
    "word_counts_french_sorted.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'tour',\n",
       " 'monde',\n",
       " 'quatre-vingts',\n",
       " 'jours',\n",
       " '']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c. Eliminate French stop words\n",
    "# Using nltk library to get French stop words and filter them out\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download French stopwords if not already available\n",
    "nltk.download('stopwords', quiet=True)\n",
    "french_stop_words = set(stopwords.words('french'))\n",
    "\n",
    "words_french_no_stopw = words_french_lower.filter(lambda w: w not in french_stop_words)\n",
    "words_french_no_stopw.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#46541]',\n",
       " '$5,000)',\n",
       " '($1',\n",
       " '(1',\n",
       " '(1,875',\n",
       " '(10,000',\n",
       " '(100,000',\n",
       " '(100,000',\n",
       " '(12,500',\n",
       " '(125,000']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d. Sort French words in alphabetical order\n",
    "# Filter empty words and sort alphabetically\n",
    "words_french_ordered = words_french_no_stopw.filter(lambda w: w != '').sortBy(lambda w: w)\n",
    "words_french_ordered.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 5389),\n",
       " ('fogg', 367),\n",
       " ('phileas', 328),\n",
       " ('plus', 320),\n",
       " ('mr.', 286),\n",
       " ('cette', 277),\n",
       " ('a', 255),\n",
       " ('passepartout', 252),\n",
       " ('répondit', 214),\n",
       " (\"qu'il\", 194)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e. Count and sort French words by frequency\n",
    "word_counts_french_clean = words_french_no_stopw.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "word_counts_french_clean_sorted = word_counts_french_clean.sortBy(lambda x: x[1], ascending=False)\n",
    "word_counts_french_clean_sorted.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'tour',\n",
       " 'monde',\n",
       " 'quatrevingts',\n",
       " 'jours',\n",
       " 'this']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f. Remove punctuation and blank spaces from French words\n",
    "# str.translate removes all punctuation from each word in the RDD\n",
    "import string\n",
    "words_french_clean = words_french_no_stopw.map(lambda w: w.translate(str.maketrans('', '', string.punctuation)))\n",
    "words_french_clean = words_french_clean.filter(lambda w: w.strip() != '')\n",
    "words_french_clean.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comparative EDA - French vs English\n",
    "\n",
    "As requested in \"Where to go from here\", we will now perform exploratory data analysis comparing the French and English versions of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARATIVE ANALYSIS: FRENCH vs ENGLISH ===\n",
      "\n",
      " MOST FREQUENT WORDS:\n",
      "\n",
      "FRENCH:\n",
      " 1.                 : 5389\n",
      " 2. fogg            :  367\n",
      " 3. phileas         :  328\n",
      " 4. plus            :  320\n",
      " 5. mr.             :  286\n",
      " 6. cette           :  277\n",
      " 7. a               :  255\n",
      " 8. passepartout    :  252\n",
      " 9. répondit        :  214\n",
      "10. qu'il           :  194\n",
      "\n",
      "ENGLISH:\n",
      " 1.                 : 2149\n",
      " 2. mr.             :  373\n",
      " 3. fogg            :  365\n",
      " 4. would           :  274\n",
      " 5. phileas         :  250\n",
      " 6. passepartout    :  239\n",
      " 7. said            :  157\n",
      " 8. could           :  139\n",
      " 9. one             :  133\n",
      "10. fogg,           :  132\n",
      "\n",
      " STATISTICAL COMPARISON:\n",
      "Metric                    French     English    \n",
      "------------------------- ---------- ---------- \n",
      "Total words               76519      68391      \n",
      "Unique words              15066      11740      \n"
     ]
    }
   ],
   "source": [
    "# Comparative analysis between French and English versions\n",
    "print(\"=== COMPARATIVE ANALYSIS: FRENCH vs ENGLISH ===\")\n",
    "\n",
    "# Use results already calculated in previous cells\n",
    "# For French: use word_counts_french_clean_sorted (from step e in French analysis)\n",
    "# For English: first ensure we have the English results by recreating if needed\n",
    "\n",
    "# Create English word count results using the same steps as French\n",
    "english_word_counts_clean = words_no_stopw.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "english_word_counts_sorted = english_word_counts_clean.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"\\n MOST FREQUENT WORDS:\")\n",
    "print(\"\\nFRENCH:\")\n",
    "french_top = word_counts_french_clean_sorted.take(10)\n",
    "for i, (word, count) in enumerate(french_top, 1):\n",
    "    print(f\"{i:2d}. {word:<15} : {count:>4}\")\n",
    "\n",
    "print(\"\\nENGLISH:\")\n",
    "english_top = english_word_counts_sorted.take(10)\n",
    "for i, (word, count) in enumerate(english_top, 1):\n",
    "    print(f\"{i:2d}. {word:<15} : {count:>4}\")\n",
    "\n",
    "# Statistical comparison using existing RDD variables\n",
    "french_unique = word_counts_french_clean_sorted.count()\n",
    "english_unique = english_word_counts_sorted.count()\n",
    "french_total = rdd_french.flatMap(lambda line: line.split(' ')).count()\n",
    "english_total = rdd.flatMap(lambda line: line.split(' ')).count()\n",
    "\n",
    "print(f\"\\n STATISTICAL COMPARISON:\")\n",
    "print(f\"{'Metric':<25} {'French':<10} {'English':<10} \")\n",
    "print(f\"{'-'*25} {'-'*10} {'-'*10} \")\n",
    "print(f\"{'Total words':<25} {french_total:<10} {english_total:<10} \")\n",
    "print(f\"{'Unique words':<25} {french_unique:<10} {english_unique:<10} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Performance Optimization and Timer\n",
    "\n",
    "As requested, we implement timing functions and test different operation orders for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING RDD PERFORMANCE\n",
      "Testing: Current word_counts\n",
      "Completed in 0.041 seconds\n",
      "Top words: [(('the', 1), 4306), (('', 1), 2149), (('of', 1), 1870)]\n"
     ]
    }
   ],
   "source": [
    "#  Python timer function for measuring RDD performance\n",
    "import time\n",
    "\n",
    "def timer(operation_name, rdd_operation):\n",
    "    # timer function to measure RDD execution time\n",
    "    print(f\"Testing: {operation_name}\")\n",
    "    start = time.time()\n",
    "    result = rdd_operation.take(10)  # Execute the RDD operation\n",
    "    end = time.time()\n",
    "    execution_time = end - start\n",
    "    print(f\"Completed in {execution_time:.3f} seconds\")\n",
    "    return result, execution_time\n",
    "\n",
    "# Test current word count RDD\n",
    "print(\"TESTING RDD PERFORMANCE\")\n",
    "result1, time1 = timer(\"Current word_counts\", words_desc)\n",
    "print(f\"Top words: {result1[:3]}\")  # Show first 3 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZING OPERATION ORDER\n",
      "PERFORMANCE RESULTS:\n",
      "Original order (a,b,c,d,e,f):  0.359 seconds\n",
      "Early filtering (b,c,f,a,e,d): 0.406 seconds\n",
      "Combined filtering (b,c+f,a,e): 0.495 seconds\n",
      "\n",
      "Fastest approach: Original order (0.359 seconds)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(\"OPTIMIZING OPERATION ORDER\")\n",
    "\n",
    "# Approach 1: Original order (a,b,c,d,e,f)\n",
    "start = time.time()\n",
    "# a. Initialize word counter with tuples (word, 1)\n",
    "words_step_a = rdd.flatMap(lambda lines: lines.split(' ')).map(lambda word: (word, 1))\n",
    "# b. Convert to lowercase \n",
    "words_step_b = rdd.flatMap(lambda lines: lines.split(' ')).map(lambda w: w.lower())\n",
    "# c. Eliminate stop words\n",
    "words_step_c = words_step_b.filter(lambda w: w not in stop_words)\n",
    "# d. Sort alphabetically and filter empty words\n",
    "words_step_d = words_step_c.filter(lambda w: w != '').sortBy(lambda w: w)\n",
    "# e. Count and sort by frequency\n",
    "word_counts_step_e = words_step_c.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "result1 = word_counts_step_e.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "# f. Remove punctuation (final step)\n",
    "words_step_f = words_step_c.map(lambda w: w.translate(str.maketrans('', '', string.punctuation)))\n",
    "words_clean_final = words_step_f.filter(lambda w: w.strip() != '')\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Approach 2: Early filtering (b,c,f,a,e,d)\n",
    "start = time.time()\n",
    "# b. Convert to lowercase first\n",
    "words_early_b = rdd.flatMap(lambda lines: lines.split(' ')).map(lambda w: w.lower())\n",
    "# c. Eliminate stop words early\n",
    "words_early_c = words_early_b.filter(lambda w: w not in stop_words)\n",
    "# f. Remove punctuation early\n",
    "words_early_f = words_early_c.map(lambda w: w.translate(str.maketrans('', '', string.punctuation)))\n",
    "words_early_f = words_early_f.filter(lambda w: w.strip() != '')\n",
    "# a. Count words with cleaned data\n",
    "words_early_a = words_early_f.map(lambda word: (word, 1))\n",
    "# e. Sort by frequency\n",
    "word_counts_early_e = words_early_a.reduceByKey(lambda a, b: a + b)\n",
    "result2 = word_counts_early_e.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "# d. Alphabetical sort (optional verification)\n",
    "words_early_d = words_early_f.sortBy(lambda w: w)\n",
    "time2 = time.time() - start\n",
    "\n",
    "# Approach 3: Combined filtering (b,c+f,a,e,d)\n",
    "start = time.time()\n",
    "# b. Convert to lowercase\n",
    "words_combined_b = rdd.flatMap(lambda lines: lines.split(' ')).map(lambda w: w.lower())\n",
    "# c+f. Combined stop words and punctuation filtering\n",
    "words_combined_cf = (words_combined_b\n",
    "                    .map(lambda w: w.translate(str.maketrans('', '', string.punctuation)))\n",
    "                    .filter(lambda w: w.strip() != '' and w not in stop_words))\n",
    "# a. Count words\n",
    "words_combined_a = words_combined_cf.map(lambda word: (word, 1))\n",
    "# e. Sort by frequency\n",
    "word_counts_combined_e = words_combined_a.reduceByKey(lambda a, b: a + b)\n",
    "result3 = word_counts_combined_e.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "# d. Alphabetical sort (verification)\n",
    "words_combined_d = words_combined_cf.sortBy(lambda w: w)\n",
    "time3 = time.time() - start\n",
    "\n",
    "# Performance comparison\n",
    "print(\"PERFORMANCE RESULTS:\")\n",
    "print(f\"Original order (a,b,c,d,e,f):  {time1:.3f} seconds\")\n",
    "print(f\"Early filtering (b,c,f,a,e,d): {time2:.3f} seconds\") \n",
    "print(f\"Combined filtering (b,c+f,a,e): {time3:.3f} seconds\")\n",
    "\n",
    "# Find fastest approach\n",
    "approaches = [(\"Original order\", time1), (\"Early filtering\", time2), (\"Combined filtering\", time3)]\n",
    "fastest = min(approaches, key=lambda x: x[1])\n",
    "print(f\"\\nFastest approach: {fastest[0]} ({fastest[1]:.3f} seconds)\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
