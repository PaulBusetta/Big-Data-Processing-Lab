{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e3674a-0e18-49ea-bf42-801c93d106a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploratory Data Analysis with Pyspark and Spark SQL\n",
    "\n",
    "The following notebook utilizes New York City taxi data from [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Load and explore nyc taxi data from january 0f 2019. The exercises can be executed using pyspark or spark sql ( a subset of the questions will be re-answered using the language not chosen for the  main work).\n",
    "- Load the zone lookup table to answer the questions about the nyc boroughs.  \n",
    "- Load nyc taxi data from January of 2025 and compare data.  \n",
    "- With any remaining time, work on the where to go from here section.  \n",
    "- Lab due date is TBD ( due dates will be updated in the readme for the class repo )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f445f856-1ce8-4dc7-88e2-90d7aed027ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the name of the new catalog\n",
    "catalog = 'taxi_eda_db'\n",
    "\n",
    "# define variables for the trips data\n",
    "schema = 'yellow_taxi_trips'\n",
    "volume = 'data'\n",
    "file_name = 'yellow_tripdata_2019-01.parquet'\n",
    "table_name = 'tbl_yellow_taxi_trips'\n",
    "path_volume = '/Volumes/' + catalog + \"/\" + schema + '/' + volume\n",
    "path_table =  catalog + \".\" + schema\n",
    "download_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "545e8186-b880-4cb8-a5f6-d73e4b407a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the catalog/schema/volume\n",
    "spark.sql('create catalog if not exists ' + catalog)\n",
    "spark.sql('create schema if not exists ' + catalog + '.' + schema)\n",
    "spark.sql('create volume if not exists ' + catalog + '.' + schema + '.' + volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8576a8-5cbe-43f6-afc0-a0c844b66755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "dbutils.fs.cp(f\"{download_url}\", f\"{path_volume}\" + \"/\" + f\"{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be82981b-8af8-4821-a20f-e1c5ffbde15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the dataframe\n",
    "df_trips = spark.read.parquet(f\"{path_volume}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bffe652-71fd-404e-913d-7f368427d7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the dataframe\n",
    "df_trips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c3add9-ff1a-4570-aa20-c65a4dc0514f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lab\n",
    "\n",
    "### Part 1\n",
    "This section can be completed either using pyspark commands or sql commands ( There will be a section after in which a self-chosen subset of the questions are re-answered using the language not used for the main section. i.e. if pyspark is chosen for the main lab, sql should be used to repeat some of the questions. )\n",
    "\n",
    "- Add a column that creates a unique key to identify each record in order to answer questions about individual trips\n",
    "- Which trip has the highest passanger count\n",
    "- What is the Average passanger count\n",
    "- Shortest/longest trip by distance? by time?\n",
    "- highest/lowest faire amounts for a trip, what burough is associated with the each.\n",
    "- busiest day/slowest single day\n",
    "- busiest/slowest time of day ( you may want to bucket these by hour or create timess such as morning, afternoon, evening, late night )\n",
    "- On average which day of the week is slowest/busiest\n",
    "- Does trip distance or num passangers affect tip amount\n",
    "- What was the highest \"extra\" charge and which trip\n",
    "- Are there any datapoints that seem to be strange/outliers (make sure to explain your reasoning in a markdown cell)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af33d24-2e54-4ccb-aa26-f0d90093064e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_id = df_trips.withColumn(\"RecordID\", monotonically_increasing_id())   \n",
    "df_with_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23bc12f4-603d-4c00-addf-f849c53ee703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, col\n",
    "\n",
    "max_value = df_with_id.agg(max(\"passenger_count\")).collect()[0][0]\n",
    "max_pass_count = df_with_id.filter(col(\"passenger_count\") == max_value)\n",
    "max_pass_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b19ec41-31c0-4c4d-978c-1a5eddf328da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "avg_value = df_with_id.agg(avg(\"passenger_count\")).collect()[0][0]\n",
    "print(\"Average passenger count: \", avg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55c072f-dd49-4b7d-8d8f-b56651a2b3fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import max, min, col\n",
    "\n",
    "# df_trip_time = df_with_id.withColumn(\n",
    "#     \"trip_time\",\n",
    "#     col(\"tpep_dropoff_datetime\") - col(\"tpep_pickup_datetime\")\n",
    "# )\n",
    "\n",
    "# max_time = df_trip_time.agg(max(\"trip_time\")).collect()[0][0]\n",
    "# min_time = df_trip_time.agg(max(\"trip_time\")).collect()[0][0]\n",
    "# max_distance = df_with_id.agg(max(\"trip_distance\")).collect()[0][0]\n",
    "# min_distance = df_with_id.agg(min(\"trip_distance\")).collect()[0][0]\n",
    "\n",
    "# max_distance_df = df_with_id.filter(col(\"trip_distance\") == max_distance)\n",
    "# min_distance_df = df_with_id.filter(col(\"trip_distance\") == min_distance)\n",
    "# max_time_df = df_trip_time.filter(col(\"trip_time\") == max_time)\n",
    "# min_time_df = df_trip_time.filter(col(\"trip_time\") == min_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3badbe-f131-4885-b4da-35b340b151b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"taxi_eda_db\"\n",
    "schema = \"yellow_taxi_trips\"\n",
    "volume = \"data\"\n",
    "file_name = \"taxi_zone_lookup.csv\"\n",
    "table_name = \"taxi_zone_lookup\"\n",
    "download_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "\n",
    "spark.sql('create catalog if not exists ' + catalog)\n",
    "spark.sql('create schema if not exists ' + catalog + '.' + schema)\n",
    "spark.sql('create volume if not exists ' + catalog + '.' + schema + '.' + volume)\n",
    "\n",
    "path_volume = f\"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "dbutils.fs.cp(download_url, f\"{path_volume}/{file_name}\")\n",
    "\n",
    "df_zones = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{path_volume}/{file_name}\")\n",
    ")\n",
    "\n",
    "df_zones.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149a320c-5e8e-4af8-baa9-559e09328775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, min, col\n",
    "\n",
    "max_fare_amount = df_with_id.agg(max(\"fare_amount\")).collect()[0][0]\n",
    "min_fare_amount = df_with_id.agg(min(\"fare_amount\")).collect()[0][0]\n",
    "\n",
    "max_fare_amount_df = df_with_id.filter(col(\"fare_amount\") == max_fare_amount).select(\"fare_amount\", \"DOLocationID\")\n",
    "min_fare_amount_df = df_with_id.filter(col(\"fare_amount\") == min_fare_amount).select(\"fare_amount\", \"DOLocationID\")\n",
    "\n",
    "df_max_joined = max_fare_amount_df.join(df_zones, max_fare_amount_df.DOLocationID == df_zones.LocationID, how=\"inner\")\n",
    "df_min_joined = min_fare_amount_df.join(df_zones, min_fare_amount_df.DOLocationID == df_zones.LocationID, how=\"inner\")\n",
    "\n",
    "df_max_joined.show(5)\n",
    "df_min_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6f66dc-8db4-4bd2-bffc-ec8cfa0f743b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df_dated = df_with_id.withColumn(\"pickup_date\", to_date(\"tpep_pickup_datetime\"))\n",
    "df_business_counts = df_dated.groupBy(\"pickup_date\").count()\n",
    "\n",
    "max_day_business = df_business_counts.agg(max(\"count\")).collect()[0][0]\n",
    "min_day_business = df_business_counts.agg(min(\"count\")).collect()[0][0]\n",
    "\n",
    "max_day_business_df = df_business_counts.filter(col(\"count\") == max_day_business)\n",
    "min_day_business_df = df_business_counts.filter(col(\"count\") == min_day_business)\n",
    "\n",
    "max_day_business_df.show(5)\n",
    "min_day_business_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786b8575-1a0d-443c-981c-818cbad8e4c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df_day_of_week = df_dated.withColumn(\"day_of_week\", date_format(\"pickup_date\", \"EEEE\"))\n",
    "df_day_of_week_counts = df_day_of_week.groupBy(\"day_of_week\").count()\n",
    "\n",
    "max_dow_business = df_day_of_week_counts.agg(max(\"count\")).collect()[0][0]\n",
    "min_dow_business = df_day_of_week_counts.agg(min(\"count\")).collect()[0][0]\n",
    "\n",
    "max_dow_business_df = df_day_of_week_counts.filter(col(\"count\") == max_dow_business)\n",
    "min_dow_business_df = df_day_of_week_counts.filter(col(\"count\") == min_dow_business)\n",
    "\n",
    "max_dow_business_df.show(5)\n",
    "min_dow_business_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82256ae3-307a-4205-b3b0-cef023f03c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "df_with_id.select(\n",
    "    corr(\"trip_distance\", \"tip_amount\").alias(\"corr_distance_tip\"),\n",
    "    corr(\"passenger_count\", \"tip_amount\").alias(\"corr_passengers_tip\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093a46c2-6461-491f-9057-a5af4414e3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, col\n",
    "\n",
    "max_extra_amount = df_with_id.agg(max(\"extra\")).collect()[0][0]\n",
    "print(f\"Max extra amount for a trip: {max_extra_amount}\")\n",
    "max_extra_amount_df = df_with_id.filter(col(\"extra\") == max_extra_amount)\n",
    "max_extra_amount_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b16dea7d-8043-4ab8-a3be-16c5736ca45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following outliers detection method is using the well known Z-Score method. It is a strong method but it assumes a normal distribution in the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dde2fd-816c-48e3-8123-8aeba4967073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def z_score_outliers(df, col_name):\n",
    "    stats = df.agg(\n",
    "        F.mean(col_name).alias(\"mean\"),\n",
    "        F.stddev(col_name).alias(\"std\")\n",
    "    ).collect()[0]\n",
    "    mean = stats[\"mean\"]\n",
    "    std = stats[\"std\"]\n",
    "    df_z_outliers = df_with_id.filter(\n",
    "        (F.col(col_name) - mean) / std > 2\n",
    "    )\n",
    "    return df_z_outliers\n",
    "\n",
    "df_z_outliers_dist = z_score_outliers(df_with_id, \"trip_distance\")\n",
    "print(\"Trip distance outliers: \")\n",
    "df_z_outliers_dist.show(5)\n",
    "\n",
    "df_z_outliers_pass = z_score_outliers(df_with_id, \"passenger_count\")\n",
    "print(\"Passengers count outliers: \")\n",
    "df_z_outliers_pass.show(5)\n",
    "\n",
    "df_z_outliers_tot = z_score_outliers(df_with_id, \"total_amount\")\n",
    "print(\"Total amount outliers: \")\n",
    "df_z_outliers_tot.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2f5628-1cbd-4195-98f2-c996ca243535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 2\n",
    "\n",
    "- Using the code for loading the first dataset as an example, load in the taxi zone lookup and answer the following questions\n",
    "- which borough had most pickups? dropoffs?\n",
    "- what are the busy/slow times by borough \n",
    "- what are the busiest days of the week by borough?\n",
    "- what is the average trip distance by borough?\n",
    "- what is the average trip fare by borough?\n",
    "- load the dataset from the most recently available january, is there a change to any of the average metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030cf918-272c-4684-b05a-ee881e1c8d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "zones = broadcast(df_zones.select(\"LocationID\", \"Borough\"))\n",
    "\n",
    "df_trips_zones = (\n",
    "    df_trips\n",
    "    .join(\n",
    "        zones.select(\n",
    "            col(\"LocationID\").alias(\"PULocationID\"),\n",
    "            col(\"Borough\").alias(\"PickupBorough\")\n",
    "        ),\n",
    "        on=\"PULocationID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        zones.select(\n",
    "            col(\"LocationID\").alias(\"DOLocationID\"),\n",
    "            col(\"Borough\").alias(\"DropoffBorough\")\n",
    "        ),\n",
    "        on=\"DOLocationID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_final = df_trips_zones.drop(\"PULocationID\", \"DOLocationID\")\n",
    "\n",
    "pickup_counts = (\n",
    "    df_final.groupBy(\"PickupBorough\")\n",
    "            .count()\n",
    "            .orderBy(col(\"count\").desc())\n",
    ")\n",
    "pickup_counts.show(1)\n",
    "\n",
    "dropoff_counts = (\n",
    "    df_final.groupBy(\"DropoffBorough\")\n",
    "            .count()\n",
    "            .orderBy(col(\"count\").desc())\n",
    ")\n",
    "dropoff_counts.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7117f31b-154e-4d53-ab20-661699407d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import hour, col, row_number\n",
    "\n",
    "df_timed = (\n",
    "    df_final\n",
    "    .withColumn(\"pickup_hour\",  hour(\"tpep_pickup_datetime\")) \n",
    "    .withColumn(\"dropoff_hour\", hour(\"tpep_dropoff_datetime\"))\n",
    ")\n",
    "\n",
    "pickup_activity = (\n",
    "    df_timed\n",
    "    .groupBy(\"PickupBorough\", \"pickup_hour\")\n",
    "    .count()\n",
    "    .orderBy(\"PickupBorough\", col(\"count\").desc())\n",
    ")\n",
    "dropoff_activity = (\n",
    "    df_timed\n",
    "    .groupBy(\"DropoffBorough\", \"dropoff_hour\")\n",
    "    .count()\n",
    "    .orderBy(\"DropoffBorough\", col(\"count\").desc())\n",
    ")\n",
    "\n",
    "w1 = Window.partitionBy(\"PickupBorough\").orderBy(col(\"count\").desc())\n",
    "busiest_pickups = (\n",
    "    pickup_activity\n",
    "    .withColumn(\"rank\", row_number().over(w1))\n",
    "    .filter(\"rank = 1\")\n",
    "    .select(\"PickupBorough\", \"pickup_hour\", \"count\")\n",
    ")\n",
    "\n",
    "w2 = Window.partitionBy(\"PickupBorough\").orderBy(col(\"count\").asc())\n",
    "slowest_pickups = (\n",
    "    pickup_activity\n",
    "    .withColumn(\"rank\", row_number().over(w2))\n",
    "    .filter(\"rank = 1\")\n",
    "    .select(\"PickupBorough\", \"pickup_hour\", \"count\")\n",
    ")\n",
    "\n",
    "busiest_pickups.show()\n",
    "slowest_pickups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375b73be-503f-4d3f-a27d-1f5f79c9ac50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import date_format, col, row_number\n",
    "\n",
    "df_days = df_final.withColumn(\"pickup_day\", date_format(\"tpep_pickup_datetime\", \"EEEE\"))\n",
    "pickup_by_day = (\n",
    "    df_days\n",
    "    .groupBy(\"PickupBorough\", \"pickup_day\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"PickupBorough\").orderBy(col(\"count\").desc())\n",
    "busiest_pickup_days = (\n",
    "    pickup_by_day\n",
    "    .withColumn(\"rank\", row_number().over(w))\n",
    "    .filter(\"rank = 1\")\n",
    "    .select(\"PickupBorough\", \"pickup_day\", \"count\")\n",
    ")\n",
    "\n",
    "busiest_pickup_days.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f95ef0d-4eb6-4f94-a540-77319ede7dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "avg_distance_pickup = (\n",
    "    df_final\n",
    "    .groupBy(\"PickupBorough\")\n",
    "    .agg(avg(\"trip_distance\").alias(\"avg_trip_distance\"))\n",
    "    .orderBy(col(\"avg_trip_distance\").desc())\n",
    ")\n",
    "\n",
    "avg_distance_pickup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53b5d86-395e-4dfb-8066-f90c4bec489c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "avg_trip_fare = (\n",
    "    df_final\n",
    "    .groupBy(\"PickupBorough\")\n",
    "    .agg(avg(\"fare_amount\").alias(\"avg_trip_fare\"))\n",
    "    .orderBy(col(\"avg_trip_fare\").desc())\n",
    ")\n",
    "\n",
    "avg_trip_fare.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3c3638-202b-41e6-bf6b-64dc0027d394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"taxi_eda_db\"\n",
    "schema = \"yellow_taxi_trips\"\n",
    "volume = \"data\"\n",
    "file_name = \"yellow_tripdata_2025-01.parquet\"\n",
    "table_name = \"tbl_yellow_taxi_trips_recent\"\n",
    "download_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\"\n",
    "\n",
    "spark.sql('create catalog if not exists ' + catalog)\n",
    "spark.sql('create schema if not exists ' + catalog + '.' + schema)\n",
    "spark.sql('create volume if not exists ' + catalog + '.' + schema + '.' + volume)\n",
    "\n",
    "path_volume = f\"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "dbutils.fs.cp(download_url, f\"{path_volume}/{file_name}\")\n",
    "\n",
    "df_taxi_trips_recent = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .parquet(f\"{path_volume}/{file_name}\")\n",
    ")\n",
    "\n",
    "df_taxi_trips_recent.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a6fdcc-247a-4da8-90b2-d7993bde09c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, lit\n",
    "\n",
    "def compute_metrics(df, label):\n",
    "    return (\n",
    "        df.agg(\n",
    "            avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "            avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "            avg(\"passenger_count\").alias(\"avg_passengers\")\n",
    "        ).withColumn(\"dataset\", lit(label))\n",
    "    )\n",
    "\n",
    "metrics_old = compute_metrics(df_trips, \"previous\")\n",
    "metrics_new = compute_metrics(df_taxi_trips_recent, \"recent\")\n",
    "\n",
    "comparison = metrics_old.unionByName(metrics_new)\n",
    "comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70500fa8-1b6f-48b8-8fec-4bb988a9f183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 3\n",
    "\n",
    "- choose 3 questions from above and re-answer them using the language you did not use for the main notebook . (i.e - if you completed the exercise in python, redo 3 questions in pure sql) . at least one of the questions to be redone must involve a join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c01e1d-bcf7-4285-95d5-d3d30a596d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_trips.createOrReplaceTempView(\"trips\")\n",
    "df_zones.createOrReplaceTempView(\"zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d24a1f6-31c3-40ee-b665-1064c9952485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  tpep_pickup_datetime,\n",
    "  tpep_dropoff_datetime,\n",
    "  timestampdiff(SECOND, tpep_pickup_datetime, tpep_dropoff_datetime) AS trip_seconds\n",
    "FROM trips;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6886bd-ef51-4ca0-bf67-35b7e3ca4580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  z.Borough AS PickupBorough,\n",
    "  AVG(t.trip_distance) AS avg_trip_distance\n",
    "FROM trips t\n",
    "JOIN zones z \n",
    "ON t.PULocationID = z.LocationID\n",
    "GROUP BY z.Borough\n",
    "ORDER BY avg_trip_distance DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29835598-29c3-4672-be22-d511a831f6da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  z.Borough AS PickupBorough,\n",
    "  AVG(t.fare_amount) AS avg_fare_amount\n",
    "FROM trips t\n",
    "JOIN zones z \n",
    "ON t.PULocationID = z.LocationID\n",
    "GROUP BY z.Borough\n",
    "ORDER BY avg_fare_amount DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4376b74-d167-4516-a30c-0f28fbed0430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Part 4\n",
    "\n",
    "As of spark v4 dataframes have native visualization support. Choose at least 3 questions from above and provide visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9a4c8a-8b86-400f-aff1-2d42daddced1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(busiest_pickups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b078c8c4-8c54-44f0-b349-8b98fa8b85a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(avg_trip_fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b5fd7e-15eb-4533-a00e-2e258dc59b05",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759944386389}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d71458-86fd-4602-aba5-cca2e9a7054f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Where to go from here\n",
    "\n",
    "- Continue building the dataset by loading in more data, start by completing the data for 2019 and calculating the busiest season (fall, winter, spring, summer)\n",
    "- Explore a dataset/datasets of your choosing"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8108289777489934,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_sparksql_and_dataframes",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}